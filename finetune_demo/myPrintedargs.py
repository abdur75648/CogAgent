from argparse import Namespace

Namespace(
    num_layers=6,
    hidden_size=1024,
    num_attention_heads=16,
    vocab_size=100,
    max_sequence_length=512,
    layernorm_order='pre',
    inner_hidden_size=None,
    hidden_size_per_attention_head=None,
    model_parallel_size=1,
    skip_init=True,
    use_gpu_initialization=False,
    num_multi_query_heads=0,
    is_gated_mlp=False,
    layernorm_epsilon=1e-05,
    hidden_dropout=0.1,
    attention_dropout=0.1,
    drop_path=0.0,
    make_vocab_size_divisible_by=128,
    experiment_name='finetune-cogagent-vqa',
    train_iters=2000,
    batch_size=1,
    lr=1e-05,
    mode='finetune',
    seed=2023,
    zero_stage=0,
    checkpoint_activations=True,
    checkpoint_num_layers=1,
    checkpoint_skip_layers=0,
    fp16=False,
    bf16=True,
    gradient_accumulation_steps=1,
    profiling=-1,
    epochs=None,
    log_interval=50,
    summary_dir='',
    save_args=False,
    lr_decay_iters=None,
    lr_decay_style='cosine',
    lr_decay_ratio=0.1,
    warmup=0.02,
    weight_decay=0.05,
    save='./checkpoints',
    load=None,
    save_interval=200,
    no_save_rng=False,
    no_load_rng=False,
    resume_dataloader=True,
    distributed_backend='nccl',
    local_rank=3,
    exit_interval=None,
    eval_batch_size=1,
    eval_iters=10,
    eval_interval=200,
    strict_eval=False,
    train_data=['../data/json/apollo_ferret_noscale.json'],
    train_data_weights=None,
    iterable_dataset=False,
    batch_from_same_dataset=False,
    valid_data=['../data/json/apollo_ferret_noscale.json'],
    test_data=None,
    split='1.',
    num_workers=1,
    block_size=10000,
    prefetch_factor=4,
    tokenizer_type='fake',
    temperature=1.0,
    top_p=0.0,
    top_k=0,
    num_beams=1,
    length_penalty=0.0,
    no_repeat_ngram_size=0,
    min_tgt_length=0,
    out_seq_length=256,
    input_source='interactive',
    output_path='./samples',
    with_id=False,
    max_inference_batch_size=12,
    device=3,
    deepspeed=True,
    deepspeed_config={
        'train_micro_batch_size_per_gpu': 1,
        'gradient_accumulation_steps': 1,
        'gradient_clipping': 0.1,
        'zero_optimization': {
            'stage': 2,
            'contiguous_gradients': False,
            'overlap_comm': True,
            'reduce_scatter': True,
            'reduce_bucket_size': 40000000.0,
            'allgather_bucket_size': 100000000.0,
            'load_from_fp32_weights': False
        },
        'offload_optimizer': {
            'device': 'cpu',
            'pin_memory': True
        },
        'zero_allow_untested_optimizer': True,
        'bf16': {
            'enabled': True
        },
        'optimizer': {
            'type': 'Adam',
            'params': {
                'lr': 1e-05,
                'betas': [0.9, 0.95],
                'eps': 1e-08,
                'weight_decay': 0.05
            }
        },
        'activation_checkpointing': {
            'partition_activations': False,
            'contiguous_memory_optimization': False,
            'cpu_checkpointing': False
        },
        'wall_clock_breakdown': False
    },
    deepscale=False,
    deepscale_config=None,
    cuda=True,
    rank=3,
    world_size=8,
    deepspeed_activation_checkpointing=True,
    master_ip='127.0.0.1',
    master_port='16666',
    max_length=1024,
    ignore_pad_token_for_loss=True,
    version='vqa',
    from_pretrained='cogagent-vqa',
    local_tokenizer='lmsys/vicuna-7b-v1.5',
    vit_checkpoint_activations=True,
    pre_seq_len=8,
    lora_rank=50,
    use_ptuning=False,
    use_lora=True,
    use_qlora=False,
    layer_range=None,
    image_length=256,
    cross_image_pix=1120,
    eva_args={},
    bos_token_id=0,
    eos_token_id=1,
    pad_token_id=-1,
    vg_token_idx=32000
)

