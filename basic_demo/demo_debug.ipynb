{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-24 16:53:29,886] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "##### Evertime #####\n",
    "import argparse\n",
    "import gradio as gr\n",
    "import os, sys\n",
    "sys.path.append(os.path.dirname(\"/workspace/CogAgent/basic_demo\"))\n",
    "world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "from sat.model.mixins import CachedAutoregressiveMixin\n",
    "from sat.mpu import get_model_parallel_world_size\n",
    "from sat.model import AutoModel\n",
    "from sat.quantization.kernels import quantize\n",
    "\n",
    "from utils.utils import chat, llama2_tokenizer, llama2_text_processor_inference, get_image_processor, parse_response, get_grounding_image_processor\n",
    "from utils.models import CogAgentModel, CogVLMModel\n",
    "\n",
    "\n",
    "DESCRIPTION = '''<h1 style='text-align: center'> <a href=\"https://github.com/THUDM/CogVLM\">CogVLM / CogAgent</a> </h1>'''\n",
    "\n",
    "NOTES = '<h3> This app is adapted from <a href=\"https://github.com/THUDM/CogVLM\">https://github.com/THUDM/CogVLM</a>. It would be recommended to check out the repo if you want to see the detail of our model, CogVLM & CogAgent. </h3>'\n",
    "\n",
    "MAINTENANCE_NOTICE1 = 'Hint 1: If the app report \"Something went wrong, connection error out\", please turn off your proxy and retry.<br>Hint 2: If you upload a large size of image like 10MB, it may take some time to upload and process. Please be patient and wait.'\n",
    "\n",
    "\n",
    "AGENT_NOTICE = 'Hint 1: To use <strong>Agent</strong> function, please use the <a href=\"https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761\">prompts for agents</a>.'\n",
    "\n",
    "GROUNDING_NOTICE = 'Hint 2: To use <strong>Grounding</strong> function, please use the <a href=\"https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L344\">prompts for grounding</a>.'\n",
    "\n",
    "default_chatbox = [(\"\", \"Hi, What do you want to know about this image?\")]\n",
    "\n",
    "\n",
    "model = image_processor = text_processor_infer = None\n",
    "\n",
    "is_grounding = False\n",
    "\n",
    "\n",
    "def process_image_without_resize(image_prompt):\n",
    "    image = Image.open(image_prompt)\n",
    "    # print(f\"height:{image.height}, width:{image.width}\")\n",
    "    timestamp = int(time.time())\n",
    "    file_ext = os.path.splitext(image_prompt)[1]\n",
    "    filename_grounding = f\"examples/{timestamp}_grounding{file_ext}\"\n",
    "    return image, filename_grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Just Once #####\n",
    "def load_model(args): \n",
    "    model, model_args = AutoModel.from_pretrained(\n",
    "        args.from_pretrained,\n",
    "        args=argparse.Namespace(\n",
    "        deepspeed=None,\n",
    "        local_rank=0,\n",
    "        rank=0,\n",
    "        world_size=world_size,\n",
    "        model_parallel_size=world_size,\n",
    "        mode='inference',\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        skip_init=True,\n",
    "        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n",
    "        device='cpu' if args.quant else 'cuda',\n",
    "        vg_token_idx = args.vg_token_idx),\n",
    "        overwrite_args={'model_parallel_size': world_size} if world_size != 1 else {}\n",
    "    )\n",
    "    model = model.eval()\n",
    "    assert world_size == get_model_parallel_world_size(), \"world size must equal to model parallel size for cli_demo!\"\n",
    "\n",
    "    language_processor_version = model_args.text_processor_version if 'text_processor_version' in model_args else args.version\n",
    "    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=language_processor_version)\n",
    "    image_processor = get_image_processor(model_args.eva_args[\"image_size\"][0])\n",
    "    cross_image_processor = get_image_processor(model_args.cross_image_pix) if \"cross_image_pix\" in model_args else None\n",
    "    grounding_image_processor = get_grounding_image_processor(args.gnd_image_pix)\n",
    "\n",
    "    if args.quant:\n",
    "        quantize(model, args.quant)\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n",
    "\n",
    "    text_processor_infer = llama2_text_processor_inference(tokenizer, args.max_length, model.image_length)\n",
    "\n",
    "    return model, image_processor, cross_image_processor, text_processor_infer, grounding_image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Evertime #####\n",
    "# def post(\n",
    "#         input_text,\n",
    "#         temperature,\n",
    "#         top_p,\n",
    "#         top_k,\n",
    "#         image_prompt,\n",
    "#         result_previous,\n",
    "#         hidden_image,\n",
    "#         state\n",
    "#         ):\n",
    "#     result_text = [(ele[0], ele[1]) for ele in result_previous]\n",
    "#     for i in range(len(result_text)-1, -1, -1):\n",
    "#         if result_text[i][0] == \"\" or result_text[i][0] == None:\n",
    "#             del result_text[i]\n",
    "#     print(f\"history {result_text}\")\n",
    "    \n",
    "#     global model, image_processor, cross_image_processor, text_processor_infer, grounding_image_processor, is_grounding\n",
    "    \n",
    "#     try:\n",
    "#         with torch.no_grad():\n",
    "#             pil_img, image_path_grounding = process_image_without_resize(image_prompt)\n",
    "#             # response, _, cache_image = chat(\n",
    "#             response, _, cache_image, bbox_outputs_dict = chat(\n",
    "#                     image_path=\"\", \n",
    "#                     model=model, \n",
    "#                     text_processor=text_processor_infer,\n",
    "#                     img_processor=image_processor,\n",
    "#                     grounding_img_processor=grounding_image_processor,\n",
    "#                     query=input_text, \n",
    "#                     history=result_text, \n",
    "#                     cross_img_processor=cross_image_processor,\n",
    "#                     image=pil_img, \n",
    "#                     max_length=2048, \n",
    "#                     top_p=top_p, \n",
    "#                     temperature=temperature,\n",
    "#                     top_k=top_k,\n",
    "#                     invalid_slices=text_processor_infer.invalid_slices if hasattr(text_processor_infer, \"invalid_slices\") else [],\n",
    "#                     no_prompt=False,\n",
    "#                     args=state['args']\n",
    "#             )\n",
    "#     except Exception as e:\n",
    "#         print(\"error message\", e)\n",
    "#         result_text.append((input_text, 'Timeout! Please wait a few minutes and retry.'))\n",
    "#         return \"\", result_text, hidden_image\n",
    "\n",
    "#     answer = response\n",
    "#     if is_grounding:\n",
    "#         parse_response(pil_img, answer, image_path_grounding)\n",
    "#         new_answer = answer.replace(input_text, \"\")\n",
    "#         result_text.append((input_text, new_answer))\n",
    "#         result_text.append((None, (image_path_grounding,)))\n",
    "#     else:\n",
    "#         result_text.append((input_text, answer))\n",
    "    \n",
    "#     print(\"Bounding box outputs: \", bbox_outputs_dict)\n",
    "    \n",
    "#     print(\"Text: \", result_text)\n",
    "#     print('finished')\n",
    "#     return \"\", result_text, hidden_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-24 16:53:33,486] [INFO] building FineTuneTrainCogAgentModelNew model ...\n",
      "[2024-03-24 16:53:33,491] [INFO] [RANK 0] > initializing model parallel with size 1\n",
      "[2024-03-24 16:53:33,492] [INFO] [RANK 0] You didn't pass in LOCAL_WORLD_SIZE environment variable. We use the guessed LOCAL_WORLD_SIZE=1. If this is wrong, please pass the LOCAL_WORLD_SIZE manually.\n",
      "[2024-03-24 16:53:33,493] [INFO] [RANK 0] You are using model-only mode.\n",
      "For torch.distributed users or loading model parallel models, set environment variables RANK, WORLD_SIZE and LOCAL_RANK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  32000\n",
      "Using VG token:  ç»™  with index:  31999\n",
      "\n",
      "\n",
      "args: Namespace(max_length=2048, top_p=0.4, top_k=1, temperature=0.8, version='chat_old', quant=None, from_pretrained='../finetune_demo/checkpoints/finetune-cogagent-vqa-03-21-19-37/', local_tokenizer='lmsys/vicuna-7b-v1.5', fp16=True, bf16=False, stream_chat=False, gnd_image_pix=512, use_lora=True, vg_token_idx=31999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded pretrained groundindino model from ../groundingdino_swinb_cogcoor.pth with msg: _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-24 16:56:24,482] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 18432088128\n",
      "INFO:sat:[RANK 0]  > number of parameters on model parallel rank 0: 18432088128\n",
      "[2024-03-24 16:57:57,847] [INFO] [RANK 0] \n",
      "\n",
      "Adding LoRA to the model for inference. Expecting the model weights to be in the LoRA format. If not, please stop the process and fix it\n",
      "\\m\n",
      "INFO:sat:[RANK 0] \n",
      "\n",
      "Adding LoRA to the model for inference. Expecting the model weights to be in the LoRA format. If not, please stop the process and fix it\n",
      "\\m\n",
      "[2024-03-24 16:57:57,851] [INFO] [RANK 0] replacing layer 0 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 0 attention with lora\n",
      "[2024-03-24 16:57:58,882] [INFO] [RANK 0] replacing layer 0 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 0 cross attention with lora\n",
      "[2024-03-24 16:57:59,670] [INFO] [RANK 0] replacing layer 1 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 1 attention with lora\n",
      "[2024-03-24 16:58:00,816] [INFO] [RANK 0] replacing layer 1 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 1 cross attention with lora\n",
      "[2024-03-24 16:58:01,571] [INFO] [RANK 0] replacing layer 2 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 2 attention with lora\n",
      "[2024-03-24 16:58:02,802] [INFO] [RANK 0] replacing layer 2 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 2 cross attention with lora\n",
      "[2024-03-24 16:58:03,477] [INFO] [RANK 0] replacing layer 3 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 3 attention with lora\n",
      "[2024-03-24 16:58:04,709] [INFO] [RANK 0] replacing layer 3 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 3 cross attention with lora\n",
      "[2024-03-24 16:58:05,470] [INFO] [RANK 0] replacing layer 4 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 4 attention with lora\n",
      "[2024-03-24 16:58:06,606] [INFO] [RANK 0] replacing layer 4 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 4 cross attention with lora\n",
      "[2024-03-24 16:58:07,370] [INFO] [RANK 0] replacing layer 5 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 5 attention with lora\n",
      "[2024-03-24 16:58:08,596] [INFO] [RANK 0] replacing layer 5 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 5 cross attention with lora\n",
      "[2024-03-24 16:58:09,277] [INFO] [RANK 0] replacing layer 6 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 6 attention with lora\n",
      "[2024-03-24 16:58:10,506] [INFO] [RANK 0] replacing layer 6 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 6 cross attention with lora\n",
      "[2024-03-24 16:58:11,270] [INFO] [RANK 0] replacing layer 7 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 7 attention with lora\n",
      "[2024-03-24 16:58:12,402] [INFO] [RANK 0] replacing layer 7 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 7 cross attention with lora\n",
      "[2024-03-24 16:58:13,077] [INFO] [RANK 0] replacing layer 8 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 8 attention with lora\n",
      "[2024-03-24 16:58:14,297] [INFO] [RANK 0] replacing layer 8 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 8 cross attention with lora\n",
      "[2024-03-24 16:58:14,970] [INFO] [RANK 0] replacing layer 9 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 9 attention with lora\n",
      "[2024-03-24 16:58:16,113] [INFO] [RANK 0] replacing layer 9 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 9 cross attention with lora\n",
      "[2024-03-24 16:58:16,870] [INFO] [RANK 0] replacing layer 10 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 10 attention with lora\n",
      "[2024-03-24 16:58:18,106] [INFO] [RANK 0] replacing layer 10 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 10 cross attention with lora\n",
      "[2024-03-24 16:58:18,870] [INFO] [RANK 0] replacing layer 11 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 11 attention with lora\n",
      "[2024-03-24 16:58:20,001] [INFO] [RANK 0] replacing layer 11 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 11 cross attention with lora\n",
      "[2024-03-24 16:58:20,678] [INFO] [RANK 0] replacing layer 12 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 12 attention with lora\n",
      "[2024-03-24 16:58:21,901] [INFO] [RANK 0] replacing layer 12 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 12 cross attention with lora\n",
      "[2024-03-24 16:58:22,577] [INFO] [RANK 0] replacing layer 13 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 13 attention with lora\n",
      "[2024-03-24 16:58:23,797] [INFO] [RANK 0] replacing layer 13 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 13 cross attention with lora\n",
      "[2024-03-24 16:58:24,570] [INFO] [RANK 0] replacing layer 14 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 14 attention with lora\n",
      "[2024-03-24 16:58:25,702] [INFO] [RANK 0] replacing layer 14 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 14 cross attention with lora\n",
      "[2024-03-24 16:58:26,472] [INFO] [RANK 0] replacing layer 15 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 15 attention with lora\n",
      "[2024-03-24 16:58:27,717] [INFO] [RANK 0] replacing layer 15 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 15 cross attention with lora\n",
      "[2024-03-24 16:58:28,473] [INFO] [RANK 0] replacing layer 16 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 16 attention with lora\n",
      "[2024-03-24 16:58:29,614] [INFO] [RANK 0] replacing layer 16 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 16 cross attention with lora\n",
      "[2024-03-24 16:58:30,371] [INFO] [RANK 0] replacing layer 17 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 17 attention with lora\n",
      "[2024-03-24 16:58:31,608] [INFO] [RANK 0] replacing layer 17 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 17 cross attention with lora\n",
      "[2024-03-24 16:58:32,370] [INFO] [RANK 0] replacing layer 18 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 18 attention with lora\n",
      "[2024-03-24 16:58:33,507] [INFO] [RANK 0] replacing layer 18 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 18 cross attention with lora\n",
      "[2024-03-24 16:58:34,272] [INFO] [RANK 0] replacing layer 19 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 19 attention with lora\n",
      "[2024-03-24 16:58:35,400] [INFO] [RANK 0] replacing layer 19 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 19 cross attention with lora\n",
      "[2024-03-24 16:58:36,169] [INFO] [RANK 0] replacing layer 20 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 20 attention with lora\n",
      "[2024-03-24 16:58:37,213] [INFO] [RANK 0] replacing layer 20 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 20 cross attention with lora\n",
      "[2024-03-24 16:58:37,970] [INFO] [RANK 0] replacing layer 21 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 21 attention with lora\n",
      "[2024-03-24 16:58:39,110] [INFO] [RANK 0] replacing layer 21 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 21 cross attention with lora\n",
      "[2024-03-24 16:58:39,873] [INFO] [RANK 0] replacing layer 22 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 22 attention with lora\n",
      "[2024-03-24 16:58:41,090] [INFO] [RANK 0] replacing layer 22 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 22 cross attention with lora\n",
      "[2024-03-24 16:58:41,777] [INFO] [RANK 0] replacing layer 23 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 23 attention with lora\n",
      "[2024-03-24 16:58:43,004] [INFO] [RANK 0] replacing layer 23 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 23 cross attention with lora\n",
      "[2024-03-24 16:58:43,770] [INFO] [RANK 0] replacing layer 24 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 24 attention with lora\n",
      "[2024-03-24 16:58:44,908] [INFO] [RANK 0] replacing layer 24 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 24 cross attention with lora\n",
      "[2024-03-24 16:58:45,679] [INFO] [RANK 0] replacing layer 25 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 25 attention with lora\n",
      "[2024-03-24 16:58:46,813] [INFO] [RANK 0] replacing layer 25 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 25 cross attention with lora\n",
      "[2024-03-24 16:58:47,570] [INFO] [RANK 0] replacing layer 26 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 26 attention with lora\n",
      "[2024-03-24 16:58:48,703] [INFO] [RANK 0] replacing layer 26 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 26 cross attention with lora\n",
      "[2024-03-24 16:58:49,471] [INFO] [RANK 0] replacing layer 27 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 27 attention with lora\n",
      "[2024-03-24 16:58:50,513] [INFO] [RANK 0] replacing layer 27 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 27 cross attention with lora\n",
      "[2024-03-24 16:58:51,370] [INFO] [RANK 0] replacing layer 28 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 28 attention with lora\n",
      "[2024-03-24 16:58:52,504] [INFO] [RANK 0] replacing layer 28 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 28 cross attention with lora\n",
      "[2024-03-24 16:58:53,277] [INFO] [RANK 0] replacing layer 29 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 29 attention with lora\n",
      "[2024-03-24 16:58:54,500] [INFO] [RANK 0] replacing layer 29 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 29 cross attention with lora\n",
      "[2024-03-24 16:58:55,176] [INFO] [RANK 0] replacing layer 30 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 30 attention with lora\n",
      "[2024-03-24 16:58:56,314] [INFO] [RANK 0] replacing layer 30 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 30 cross attention with lora\n",
      "[2024-03-24 16:58:57,070] [INFO] [RANK 0] replacing layer 31 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 31 attention with lora\n",
      "[2024-03-24 16:58:58,312] [INFO] [RANK 0] replacing layer 31 cross attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 31 cross attention with lora\n",
      "[2024-03-24 16:58:59,078] [INFO] [RANK 0] replacing layer 0 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 0 attention with lora\n",
      "[2024-03-24 16:58:59,875] [INFO] [RANK 0] replacing layer 1 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 1 attention with lora\n",
      "[2024-03-24 16:59:00,577] [INFO] [RANK 0] replacing layer 2 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 2 attention with lora\n",
      "[2024-03-24 16:59:01,280] [INFO] [RANK 0] replacing layer 3 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 3 attention with lora\n",
      "[2024-03-24 16:59:01,985] [INFO] [RANK 0] replacing layer 4 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 4 attention with lora\n",
      "[2024-03-24 16:59:02,778] [INFO] [RANK 0] replacing layer 5 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 5 attention with lora\n",
      "[2024-03-24 16:59:03,484] [INFO] [RANK 0] replacing layer 6 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 6 attention with lora\n",
      "[2024-03-24 16:59:04,183] [INFO] [RANK 0] replacing layer 7 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 7 attention with lora\n",
      "[2024-03-24 16:59:04,981] [INFO] [RANK 0] replacing layer 8 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 8 attention with lora\n",
      "[2024-03-24 16:59:05,670] [INFO] [RANK 0] replacing layer 9 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 9 attention with lora\n",
      "[2024-03-24 16:59:06,378] [INFO] [RANK 0] replacing layer 10 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 10 attention with lora\n",
      "[2024-03-24 16:59:07,169] [INFO] [RANK 0] replacing layer 11 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 11 attention with lora\n",
      "[2024-03-24 16:59:07,970] [INFO] [RANK 0] replacing layer 12 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 12 attention with lora\n",
      "[2024-03-24 16:59:08,676] [INFO] [RANK 0] replacing layer 13 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 13 attention with lora\n",
      "[2024-03-24 16:59:09,469] [INFO] [RANK 0] replacing layer 14 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 14 attention with lora\n",
      "[2024-03-24 16:59:10,178] [INFO] [RANK 0] replacing layer 15 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 15 attention with lora\n",
      "[2024-03-24 16:59:10,878] [INFO] [RANK 0] replacing layer 16 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 16 attention with lora\n",
      "[2024-03-24 16:59:11,677] [INFO] [RANK 0] replacing layer 17 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 17 attention with lora\n",
      "[2024-03-24 16:59:12,380] [INFO] [RANK 0] replacing layer 18 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 18 attention with lora\n",
      "[2024-03-24 16:59:13,078] [INFO] [RANK 0] replacing layer 19 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 19 attention with lora\n",
      "[2024-03-24 16:59:13,788] [INFO] [RANK 0] replacing layer 20 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 20 attention with lora\n",
      "[2024-03-24 16:59:14,478] [INFO] [RANK 0] replacing layer 21 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 21 attention with lora\n",
      "[2024-03-24 16:59:15,174] [INFO] [RANK 0] replacing layer 22 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 22 attention with lora\n",
      "[2024-03-24 16:59:15,874] [INFO] [RANK 0] replacing layer 23 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 23 attention with lora\n",
      "[2024-03-24 16:59:16,577] [INFO] [RANK 0] replacing layer 24 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 24 attention with lora\n",
      "[2024-03-24 16:59:17,287] [INFO] [RANK 0] replacing layer 25 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 25 attention with lora\n",
      "[2024-03-24 16:59:17,978] [INFO] [RANK 0] replacing layer 26 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 26 attention with lora\n",
      "[2024-03-24 16:59:18,781] [INFO] [RANK 0] replacing layer 27 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 27 attention with lora\n",
      "[2024-03-24 16:59:19,569] [INFO] [RANK 0] replacing layer 28 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 28 attention with lora\n",
      "[2024-03-24 16:59:20,270] [INFO] [RANK 0] replacing layer 29 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 29 attention with lora\n",
      "[2024-03-24 16:59:20,973] [INFO] [RANK 0] replacing layer 30 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 30 attention with lora\n",
      "[2024-03-24 16:59:21,682] [INFO] [RANK 0] replacing layer 31 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 31 attention with lora\n",
      "[2024-03-24 16:59:22,479] [INFO] [RANK 0] replacing layer 32 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 32 attention with lora\n",
      "[2024-03-24 16:59:23,182] [INFO] [RANK 0] replacing layer 33 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 33 attention with lora\n",
      "[2024-03-24 16:59:24,069] [INFO] [RANK 0] replacing layer 34 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 34 attention with lora\n",
      "[2024-03-24 16:59:24,769] [INFO] [RANK 0] replacing layer 35 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 35 attention with lora\n",
      "[2024-03-24 16:59:25,474] [INFO] [RANK 0] replacing layer 36 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 36 attention with lora\n",
      "[2024-03-24 16:59:26,170] [INFO] [RANK 0] replacing layer 37 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 37 attention with lora\n",
      "[2024-03-24 16:59:26,880] [INFO] [RANK 0] replacing layer 38 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 38 attention with lora\n",
      "[2024-03-24 16:59:27,577] [INFO] [RANK 0] replacing layer 39 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 39 attention with lora\n",
      "[2024-03-24 16:59:28,291] [INFO] [RANK 0] replacing layer 40 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 40 attention with lora\n",
      "[2024-03-24 16:59:28,978] [INFO] [RANK 0] replacing layer 41 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 41 attention with lora\n",
      "[2024-03-24 16:59:29,685] [INFO] [RANK 0] replacing layer 42 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 42 attention with lora\n",
      "[2024-03-24 16:59:30,379] [INFO] [RANK 0] replacing layer 43 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 43 attention with lora\n",
      "[2024-03-24 16:59:31,175] [INFO] [RANK 0] replacing layer 44 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 44 attention with lora\n",
      "[2024-03-24 16:59:31,874] [INFO] [RANK 0] replacing layer 45 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 45 attention with lora\n",
      "[2024-03-24 16:59:32,579] [INFO] [RANK 0] replacing layer 46 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 46 attention with lora\n",
      "[2024-03-24 16:59:33,369] [INFO] [RANK 0] replacing layer 47 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 47 attention with lora\n",
      "[2024-03-24 16:59:34,772] [INFO] [RANK 0] replacing layer 48 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 48 attention with lora\n",
      "[2024-03-24 16:59:35,679] [INFO] [RANK 0] replacing layer 49 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 49 attention with lora\n",
      "[2024-03-24 16:59:36,472] [INFO] [RANK 0] replacing layer 50 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 50 attention with lora\n",
      "[2024-03-24 16:59:37,170] [INFO] [RANK 0] replacing layer 51 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 51 attention with lora\n",
      "[2024-03-24 16:59:37,877] [INFO] [RANK 0] replacing layer 52 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 52 attention with lora\n",
      "[2024-03-24 16:59:38,578] [INFO] [RANK 0] replacing layer 53 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 53 attention with lora\n",
      "[2024-03-24 16:59:39,374] [INFO] [RANK 0] replacing layer 54 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 54 attention with lora\n",
      "[2024-03-24 16:59:40,070] [INFO] [RANK 0] replacing layer 55 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 55 attention with lora\n",
      "[2024-03-24 16:59:40,774] [INFO] [RANK 0] replacing layer 56 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 56 attention with lora\n",
      "[2024-03-24 16:59:41,477] [INFO] [RANK 0] replacing layer 57 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 57 attention with lora\n",
      "[2024-03-24 16:59:42,184] [INFO] [RANK 0] replacing layer 58 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 58 attention with lora\n",
      "[2024-03-24 16:59:42,877] [INFO] [RANK 0] replacing layer 59 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 59 attention with lora\n",
      "[2024-03-24 16:59:43,577] [INFO] [RANK 0] replacing layer 60 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 60 attention with lora\n",
      "[2024-03-24 16:59:44,277] [INFO] [RANK 0] replacing layer 61 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 61 attention with lora\n",
      "[2024-03-24 16:59:45,069] [INFO] [RANK 0] replacing layer 62 attention with lora\n",
      "INFO:sat:[RANK 0] replacing layer 62 attention with lora\n",
      "[2024-03-24 16:59:45,825] [INFO] [RANK 0] global rank 0 is loading checkpoint ../finetune_demo/checkpoints/finetune-cogagent-vqa-03-21-19-37/3000/mp_rank_00_model_states.pt\n",
      "INFO:sat:[RANK 0] global rank 0 is loading checkpoint ../finetune_demo/checkpoints/finetune-cogagent-vqa-03-21-19-37/3000/mp_rank_00_model_states.pt\n",
      "[2024-03-24 17:04:35,970] [INFO] [RANK 0] > successfully loaded ../finetune_demo/checkpoints/finetune-cogagent-vqa-03-21-19-37/3000/mp_rank_00_model_states.pt\n",
      "INFO:sat:[RANK 0] > successfully loaded ../finetune_demo/checkpoints/finetune-cogagent-vqa-03-21-19-37/3000/mp_rank_00_model_states.pt\n"
     ]
    }
   ],
   "source": [
    "##### Just Once #####\n",
    "global model, image_processor, cross_image_processor, text_processor_infer, grounding_image_processor, is_grounding\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    max_length=2048,\n",
    "    top_p=0.4,\n",
    "    top_k=1,\n",
    "    temperature=0.8,\n",
    "    version=\"chat_old\",\n",
    "    quant=None,\n",
    "    from_pretrained=\"../finetune_demo/checkpoints/finetune-cogagent-vqa-03-21-19-37/\",\n",
    "    local_tokenizer=\"lmsys/vicuna-7b-v1.5\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    stream_chat=False,\n",
    "    gnd_image_pix=512,\n",
    "    use_lora=False\n",
    ")\n",
    "args.fp16 = True\n",
    "args.use_lora = True\n",
    "rank = int(os.environ.get('RANK', 0))\n",
    "world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "from utils.utils import llama2_tokenizer\n",
    "tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)\n",
    "vg_token = \"ç»™\"\n",
    "args.vg_token_idx = tokenizer.convert_tokens_to_ids(vg_token)\n",
    "print(\"Total number of tokens: \", tokenizer.vocab_size)\n",
    "print(\"Using VG token: \", vg_token, \" with index: \", args.vg_token_idx)\n",
    "print(\"\\n\\nargs:\", args)\n",
    "assert args.use_lora == True\n",
    "\n",
    "model, image_processor, cross_image_processor, text_processor_infer, grounding_image_processor = load_model(args)\n",
    "is_grounding = 'grounding' in args.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-24 17:04:38,988] [INFO] [RANK 0] Processing image...\n",
      "INFO:sat:[RANK 0] Processing image...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1920x1080 at 0x7F69D581A710>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded image and text\n",
    "image_prompt = 'wf_deep13_g.png'\n",
    "input_text = 'Task: Give me the list of 30 software engineer working in a Venture firm with minimum 10 years of experience. \\n Previous Action: TYPE: Type Software engineer in search for a job title tab \\nGive me the next action?'\n",
    "input_text += vg_token\n",
    "\n",
    "result_text = []\n",
    "hidden_image = None\n",
    "state = {'args': args}  # Assuming args is defined\n",
    "\n",
    "with torch.no_grad():\n",
    "    pil_img, image_path_grounding = process_image_without_resize(image_prompt)\n",
    "    text_processor,output, _, cache_image, bbox_outputs_dict = chat(\n",
    "            image_path=\"\", \n",
    "            model=model, \n",
    "            text_processor=text_processor_infer,\n",
    "            img_processor=image_processor,\n",
    "            grounding_img_processor=grounding_image_processor,\n",
    "            query=input_text, \n",
    "            history=result_text, \n",
    "            cross_img_processor=cross_image_processor,\n",
    "            image=pil_img, \n",
    "            max_length=2048, \n",
    "            top_p=0.4, \n",
    "            temperature=0.8,\n",
    "            top_k=10,\n",
    "            invalid_slices=text_processor_infer.invalid_slices if hasattr(text_processor_infer, \"invalid_slices\") else [],\n",
    "            no_prompt=False,\n",
    "            args=state['args']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bbox_outputs': {'pred_logits': tensor([[[-1.8351,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "           [-1.6891,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "           [-1.8817,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "           ...,\n",
       "           [-3.6626,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "           [-3.5564,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "           [-2.7744,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "         device='cuda:0'),\n",
       "  'pred_boxes': tensor([[[0.2758, 0.5381, 0.1375, 0.0372],\n",
       "           [0.3659, 0.4159, 0.1684, 0.0379],\n",
       "           [0.2830, 0.6452, 0.1699, 0.0460],\n",
       "           ...,\n",
       "           [0.2157, 0.1857, 0.0718, 0.0278],\n",
       "           [0.4865, 0.7780, 0.1891, 0.0452],\n",
       "           [0.8285, 0.4096, 0.1110, 0.0324]]], device='cuda:0'),\n",
       "  'text_mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False]], device='cuda:0'),\n",
       "  'aux_outputs': [{'pred_logits': tensor([[[-2.2889,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-2.4526,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-2.4769,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             ...,\n",
       "             [-3.5645,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-4.0083,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.4724,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "           device='cuda:0'),\n",
       "    'pred_boxes': tensor([[[0.2798, 0.5374, 0.1458, 0.0439],\n",
       "             [0.3654, 0.4154, 0.1659, 0.0401],\n",
       "             [0.2841, 0.6456, 0.1727, 0.0435],\n",
       "             ...,\n",
       "             [0.2129, 0.1881, 0.0765, 0.0340],\n",
       "             [0.4937, 0.7735, 0.2110, 0.0461],\n",
       "             [0.8276, 0.4094, 0.1065, 0.0329]]], device='cuda:0')},\n",
       "   {'pred_logits': tensor([[[-1.7286,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.8603,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.9207,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             ...,\n",
       "             [-3.6328,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.5086,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.2623,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "           device='cuda:0'),\n",
       "    'pred_boxes': tensor([[[0.2766, 0.5370, 0.1404, 0.0409],\n",
       "             [0.3646, 0.4159, 0.1608, 0.0377],\n",
       "             [0.2824, 0.6444, 0.1657, 0.0441],\n",
       "             ...,\n",
       "             [0.2139, 0.1860, 0.0742, 0.0302],\n",
       "             [0.4932, 0.7746, 0.1959, 0.0453],\n",
       "             [0.8293, 0.4076, 0.1057, 0.0318]]], device='cuda:0')},\n",
       "   {'pred_logits': tensor([[[-1.7584,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.8495,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.8767,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             ...,\n",
       "             [-3.7975,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.8539,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.0420,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "           device='cuda:0'),\n",
       "    'pred_boxes': tensor([[[0.2794, 0.5378, 0.1456, 0.0394],\n",
       "             [0.3665, 0.4168, 0.1683, 0.0369],\n",
       "             [0.2858, 0.6454, 0.1715, 0.0452],\n",
       "             ...,\n",
       "             [0.2149, 0.1869, 0.0758, 0.0283],\n",
       "             [0.4960, 0.7758, 0.2103, 0.0470],\n",
       "             [0.8281, 0.4094, 0.1095, 0.0320]]], device='cuda:0')},\n",
       "   {'pred_logits': tensor([[[-1.5076,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.4086,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.5233,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             ...,\n",
       "             [-3.4525,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.3355,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-2.5195,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "           device='cuda:0'),\n",
       "    'pred_boxes': tensor([[[0.2759, 0.5382, 0.1361, 0.0370],\n",
       "             [0.3663, 0.4165, 0.1653, 0.0367],\n",
       "             [0.2833, 0.6456, 0.1675, 0.0445],\n",
       "             ...,\n",
       "             [0.2151, 0.1859, 0.0713, 0.0276],\n",
       "             [0.4925, 0.7774, 0.1988, 0.0458],\n",
       "             [0.8288, 0.4097, 0.1092, 0.0323]]], device='cuda:0')},\n",
       "   {'pred_logits': tensor([[[-1.8610,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.7468,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-1.8837,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             ...,\n",
       "             [-3.7618,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-3.5361,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "             [-2.7966,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "           device='cuda:0'),\n",
       "    'pred_boxes': tensor([[[0.2757, 0.5380, 0.1366, 0.0369],\n",
       "             [0.3657, 0.4160, 0.1666, 0.0375],\n",
       "             [0.2828, 0.6452, 0.1687, 0.0456],\n",
       "             ...,\n",
       "             [0.2157, 0.1859, 0.0712, 0.0279],\n",
       "             [0.4860, 0.7769, 0.1867, 0.0451],\n",
       "             [0.8285, 0.4096, 0.1105, 0.0326]]], device='cuda:0')}],\n",
       "  'interm_outputs': {'pred_logits': tensor([[[-1.6093,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-1.8435,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-1.8945,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            ...,\n",
       "            [-3.8564,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-3.8576,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-3.8590,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_boxes': tensor([[[0.2853, 0.5373, 0.1576, 0.0392],\n",
       "            [0.3633, 0.4146, 0.1643, 0.0378],\n",
       "            [0.2847, 0.6449, 0.1753, 0.0407],\n",
       "            ...,\n",
       "            [0.2115, 0.1857, 0.0708, 0.0333],\n",
       "            [0.5034, 0.7739, 0.2112, 0.0402],\n",
       "            [0.8313, 0.4067, 0.1065, 0.0306]]], device='cuda:0')},\n",
       "  'interm_outputs_for_matching_pre': {'pred_logits': tensor([[[-1.6093,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-1.8435,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-1.8945,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            ...,\n",
       "            [-3.8564,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-3.8576,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "            [-3.8590,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_boxes': tensor([[[0.2891, 0.5391, 0.0500, 0.0500],\n",
       "            [0.3672, 0.4141, 0.0500, 0.0500],\n",
       "            [0.2891, 0.6484, 0.0500, 0.0500],\n",
       "            ...,\n",
       "            [0.2109, 0.1797, 0.0500, 0.0500],\n",
       "            [0.5078, 0.7734, 0.0500, 0.0500],\n",
       "            [0.8359, 0.3984, 0.0500, 0.0500]]], device='cuda:0')}},\n",
       " 'gt_ids': [0],\n",
       " 'target': [{'boxes': tensor([0, 0, 0, 0], device='cuda:0'),\n",
       "   'labels': tensor([0], device='cuda:0')}],\n",
       " 'initial_pred_embeddings': tensor([[ 2.3535e-01,  3.2559e+00,  5.7324e-01, -8.9014e-01,  1.6768e+00,\n",
       "           6.0938e+00, -4.5078e+00,  4.9341e-01,  7.1016e+00, -5.4727e+00,\n",
       "          -6.7969e-01,  4.2993e-01,  4.6436e-01, -2.1250e+00,  1.2393e+00,\n",
       "           1.4375e+00,  1.3213e+00,  3.9673e-02,  1.2610e-01,  2.7954e-01,\n",
       "          -1.5449e+00, -3.8008e+00, -6.2256e-01, -9.5703e-01,  2.6621e+00,\n",
       "           2.5391e+00, -3.1602e+00, -4.7734e+00,  1.3018e+00, -2.7129e+00,\n",
       "          -1.0760e-01,  5.0352e+00,  2.7637e+00,  9.1455e-01, -5.9766e-01,\n",
       "           1.6748e+00, -2.2207e+00,  1.9800e-01,  2.0430e+00,  3.9575e-01,\n",
       "           1.1045e+00, -5.1660e-01,  1.3066e+00,  9.3896e-01,  5.9883e+00,\n",
       "           6.2744e-01, -7.4297e+00,  3.2642e-01,  1.1023e-01, -2.2148e+00,\n",
       "          -3.5352e+00,  4.5898e+00,  1.5100e-01,  5.2188e+00, -8.7354e-01,\n",
       "           1.1016e+00,  4.8145e-01,  4.2383e+00,  3.0586e+00, -5.8289e-02,\n",
       "          -5.0820e+00, -9.2041e-01,  1.4258e+00,  1.5410e+00, -9.1736e-02,\n",
       "          -7.3086e+00, -1.8857e+00,  6.1836e+00, -3.1719e+00, -9.2285e-01,\n",
       "          -3.5000e+00,  8.1641e+00,  6.3965e-02, -1.5781e+00,  1.8799e+00,\n",
       "          -3.6309e+00, -1.0889e-01,  3.6250e+00,  3.8696e-01, -5.6738e-01,\n",
       "          -1.7080e+00, -5.3125e+00,  1.0947e+00, -1.3906e+00, -1.4873e+00,\n",
       "           6.2158e-01, -1.4268e+00, -5.2393e-01,  5.5117e+00, -4.6328e+00,\n",
       "           2.4043e+00,  3.0469e+00, -4.2344e+00, -7.0752e-01,  1.4014e+00,\n",
       "           1.8291e+00,  1.5469e+00,  1.1436e+00, -9.9141e+00, -5.6787e-01,\n",
       "           1.3604e+00,  3.7754e+00, -2.1855e+00,  4.6992e+00, -2.7012e+00,\n",
       "          -1.0254e+00, -4.2175e-02,  2.1289e-01,  3.2969e+00,  9.4238e-01,\n",
       "          -1.4395e+00,  2.1562e+00,  1.4219e+00,  4.5625e+00,  3.5254e+00,\n",
       "          -1.2217e+00,  1.2754e+00,  1.4922e+00,  3.1641e+00, -9.7070e-01,\n",
       "           7.4961e+00, -4.7148e+00,  4.4336e+00,  8.2568e-01,  2.9810e-01,\n",
       "           5.5781e+00, -7.6965e-02,  9.5625e+00, -5.3086e+00,  4.7607e-03,\n",
       "           2.9824e+00,  2.6230e+00,  5.5820e+00, -1.5771e-01, -1.1240e+00,\n",
       "          -5.4258e+00, -8.0391e+00,  3.4492e+00, -2.1113e+00, -9.8291e-01,\n",
       "          -3.1274e-01, -5.4805e+00, -2.9082e+00, -6.5781e+00,  3.8086e+00,\n",
       "           8.6230e-01,  3.5332e+00, -7.5928e-01, -3.8223e+00,  3.8208e-01,\n",
       "          -2.3730e+00, -4.3008e+00, -4.2871e-01,  1.0117e+00, -8.5742e-01,\n",
       "           2.6025e-01,  5.2295e-01,  5.1523e+00, -9.1992e-01, -4.3604e-01,\n",
       "           3.7910e+00,  3.8711e+00,  2.8438e+00,  6.3770e-01,  5.0547e+00,\n",
       "           3.1250e+00, -1.5918e-01, -5.4414e+00,  3.2166e-02, -4.1523e+00,\n",
       "          -3.7158e-01, -2.0352e+00, -2.5039e+00,  2.5371e+00,  1.4023e+00,\n",
       "          -1.1074e+00, -1.8867e+00, -7.1836e+00,  6.1328e-01, -1.1299e+00,\n",
       "          -3.2617e-01, -3.5474e-01,  2.9844e+00,  9.3701e-01,  1.9150e+00,\n",
       "          -3.1074e+00,  1.1729e+00,  3.8672e+00,  1.5977e+00,  3.7129e+00,\n",
       "          -5.5542e-02,  3.7598e-01, -1.3306e-01, -6.4453e-01,  1.9766e+00,\n",
       "           4.8633e-01, -4.8867e+00, -2.3071e-02,  3.6680e+00, -7.8662e-01,\n",
       "           2.4258e+00, -1.2051e+00, -4.1992e+00, -3.9629e+00, -2.6914e+00,\n",
       "          -1.9229e+00,  4.2617e+00, -1.4434e+00,  6.7480e-01, -2.7771e-02,\n",
       "           1.6348e+00, -3.2617e-01, -1.6577e-01, -4.9297e+00,  5.4980e-01,\n",
       "          -3.4688e+00,  2.9492e+00,  1.2188e+00, -1.6709e+00,  5.0977e-01,\n",
       "          -6.5430e+00, -3.2168e+00,  2.2246e+00, -3.6270e+00,  2.7188e+00,\n",
       "           1.0527e+00,  4.0742e+00,  2.9512e+00, -9.7656e-01,  1.6250e+00,\n",
       "          -1.5732e+00, -2.3203e+00,  1.3398e+00,  4.3750e-01, -3.6953e+00,\n",
       "           2.0293e+00, -3.0029e-01,  2.0469e+00,  5.0293e-01, -2.2168e+00,\n",
       "          -5.6719e+00,  4.4219e+00, -3.8848e+00,  2.5645e+00, -1.8584e+00,\n",
       "          -2.3848e+00, -1.1748e+00, -1.6719e+00, -3.0117e+00,  3.5645e+00,\n",
       "           5.9297e+00,  4.7188e+00,  1.0801e+00,  3.2695e+00, -6.5479e-01,\n",
       "          -6.3379e-01]], device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_outputs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 319, 32000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape # torch.Size([1, 319, 32000])\n",
    "# output = output.squeeze(0)\n",
    "# output.shape # torch.Size([319, 32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "# print(output.shape) # torch.Size([319, 32000])\n",
    "# before passing to decoder Convert output to a list of integers\n",
    "out = output.squeeze(0).tolist()\n",
    "print(len(out))\n",
    "print(len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output.shape) # torch.Size([1, 319, 32000])\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 319])\n",
      "319\n",
      "nobody sierp letter letter Q hren   includes of of Q search interface filter. interface home.. search....hren.<s>. include job. include. Comp Q. H............E job. H include. jobhren. includes include includeshren Bo Bo data Pl include. search include type. type... include.. Bo.ed... include include Bo. type job include job. includes Bo job no job job job job type job De include job. type.. blue plays Eng Bo software Software Is Is W we job W W job NR Job Bo. include Bo job De De job search includes software software Bo Bo Bo Bo Bo job App Bo De Bo pop Bo W NR NR NR NR Is Is Is Is Eng Anal green Bo button software Is Is Is Is Is Is Is software Is Is. A Is inv In Bo Bo Is Is Is Is Is Is Is Is Is and no pop inv Bo A Me I Is Is Bo Is I Is Is interface. no Is Inv Is include software Bo I Bo Bo I Bo Is.. Comped job Q Is bo Bo I Is Is Is I Is Is Bo buttonshren In A A No A A Is Is example Is include A. Is buttons search8</s>: We the a information of people5 people people engine profiles in a companyure Capital.  5 year years of experience.\n",
      " Search Searchlim task: WeYPE: We Software Engine in the filter job job title tabç»™ç»™ç»™iven</s> Re Re action: Re</s> The Re\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(output, dim=-1)\n",
    "print(token_ids.shape)\n",
    "token_ids = token_ids.squeeze().tolist()\n",
    "print(len(token_ids))\n",
    "text = tokenizer.decode(token_ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 319])\n",
      "319\n",
      "nobody sierp letter letter Q hren   includes of of Q search interface filter. interface home.. search....hren.<s>. include job. include. Comp Q. H............E job. H include. jobhren. includes include includeshren Bo Bo data Pl include. search include type. type... include.. Bo.ed... include include Bo. type job include job. includes Bo job no job job job job type job De include job. type.. blue plays Eng Bo software Software Is Is W we job W W job NR Job Bo. include Bo job De De job search includes software software Bo Bo Bo Bo Bo job App Bo De Bo pop Bo W NR NR NR NR Is Is Is Is Eng Anal green Bo button software Is Is Is Is Is Is Is software Is Is. A Is inv In Bo Bo Is Is Is Is Is Is Is Is Is and no pop inv Bo A Me I Is Is Bo Is I Is Is interface. no Is Inv Is include software Bo I Bo Bo I Bo Is.. Comped job Q Is bo Bo I Is Is Is I Is Is Bo buttonshren In A A No A A Is Is example Is include A. Is buttons search8</s>: We the a information of people5 people people engine profiles in a companyure Capital.  5 year years of experience.\n",
      " Search Searchlim task: WeYPE: We Software Engine in the filter job job title tabç»™ç»™ç»™iven</s> Re Re action: Re</s> The Re\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "probabilities = F.softmax(output, dim=-1)\n",
    "token_ids = torch.argmax(probabilities, dim=-1)\n",
    "print(token_ids.shape)\n",
    "token_ids = token_ids.squeeze().tolist()\n",
    "print(len(token_ids))\n",
    "text = tokenizer.decode(token_ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
